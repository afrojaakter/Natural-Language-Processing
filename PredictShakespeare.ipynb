{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PredictShakespeare.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNB3CB/1S3xSG17phxLLtZC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afrojaakter/Natural-Language-Processing/blob/main/PredictShakespeare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1L0-oDRR-tp"
      },
      "source": [
        "We will built a language model to train it on Cloud TPU. This model will predict the next character of text given the text so far. The trained model can generate new snippets of text that read in a similar style to the test training data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDR-xCeUmIxc"
      },
      "source": [
        "###Data, model and training:\n",
        "\n",
        "We will train a model on the combined work of William Shakespeare, then use the model to compose a play in the style of *The Great Bard*:\n",
        "\n",
        "<blockquote>\n",
        "Loves that led me no dumbs lack her Berjoy's face with her to-day.  \n",
        "The spirits roar'd; which shames which within his powers  \n",
        "\tWhich tied up remedies lending with occasion,  \n",
        "A loud and Lancaster, stabb'd in me  \n",
        "\tUpon my sword for ever: 'Agripo'er, his days let me free.  \n",
        "\tStop it of that word, be so: at Lear,  \n",
        "\tWhen I did profess the hour-stranger for my life,  \n",
        "\tWhen I did sink to be cried how for aught;  \n",
        "\tSome beds which seeks chaste senses prove burning;  \n",
        "But he perforces seen in her eyes so fast;  \n",
        "And _  \n",
        "</blockquote>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVE9lWC9nE3N"
      },
      "source": [
        "####Download data\n",
        "We will get *The completer works of William Shakespeeare* as a single text file from [Project Gutenberg](https://www.gutenberg.org/). We will use snippets from this file as training data. The target snippet is offset by one character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDaMWS4RoGGn",
        "outputId": "c9b7923d-d304-4e9a-9945-21e8bd16b8e9"
      },
      "source": [
        "#download data\n",
        "!wget --show-progress --continue -O /content/shakespeare.txt http://www.gutenberg.org/files/100/100-0.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-14 18:26:33--  http://www.gutenberg.org/files/100/100-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.gutenberg.org/files/100/100-0.txt [following]\n",
            "--2021-07-14 18:26:33--  https://www.gutenberg.org/files/100/100-0.txt\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5757108 (5.5M) [text/plain]\n",
            "Saving to: ‘/content/shakespeare.txt’\n",
            "\n",
            "/content/shakespear 100%[===================>]   5.49M  18.5MB/s    in 0.3s    \n",
            "\n",
            "2021-07-14 18:26:34 (18.5 MB/s) - ‘/content/shakespeare.txt’ saved [5757108/5757108]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJrxN7S2Ryic",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c57dc301-69c6-47df-de93-269ac5c292d0"
      },
      "source": [
        "#visualizing some input dataset\n",
        "!head -n5 /content/shakespeare.txt\n",
        "!echo \"...\"\n",
        "!shuf -n5 /content/shakespeare.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "﻿The Project Gutenberg eBook of The Complete Works of William Shakespeare, by William Shakespeare\r\n",
            "\r\n",
            "This eBook is for the use of anyone anywhere in the United States and\r\n",
            "most other parts of the world at no cost and with almost no restrictions\r\n",
            "whatsoever. You may copy it, give it away or re-use it under the terms\r\n",
            "...\n",
            "With marriage wherefore was he mock’d,\n",
            "    What dost thou, or what art thou, Angelo?\n",
            "SIR TOBY.\n",
            "\n",
            "Imagine her as one in dead of night\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHasgUhIo_fZ"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "import distutils\n",
        "if distutils.version.LooseVersion(tf.__version__) < '2.0':\n",
        "  raise Exception(\"This notebook is compatible with Tensorflow 2.0 or higher.\")\n",
        "\n",
        "ShakespeareTxt = 'shakespeare.txt'\n",
        "\n",
        "def transform(txt):\n",
        "  return np.asarray([ord(c) for c in txt if ord(c) < 255], dtype = np.int32)\n",
        "\n",
        "def input_fun(seq_len = 100, batch_size = 1024):\n",
        "  \"\"\"Return a dataset of source and target sequences for training.\"\"\"\n",
        "  with tf.io.gfile.GFile(ShakespeareTxt, 'r') as f:\n",
        "    txt = f.read()\n",
        "  \n",
        "  source = tf.constant(transform(txt), dtype = tf.int32)\n",
        "\n",
        "  ds = tf.data.Dataset.from_tensor_slices(source).batch(seq_len + 1, drop_remainder = True)\n",
        "\n",
        "  def split_input_target(chunk):\n",
        "    \"\"\"Slicing the data into input (all but the last) and target (the last number) from the chunk\"\"\"\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "  \n",
        "  buffer_size = 10000\n",
        "  ds = ds.map(split_input_target).shuffle(buffer_size).batch(batch_size, drop_remainder = True)\n",
        "\n",
        "  return ds.repeat()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4agHVK1Ot0L"
      },
      "source": [
        "###Model\n",
        "we will use two-layer, forward-LSTM.\n",
        "\n",
        "Input dim of the Emdedding layer is 256, as the vocabulary size is 256.\n",
        "\n",
        "During training we will make sure that ```stateful=False``` because we want to reset the state of the trained model, we want ```stateful=True``` so that the model can retain information across the current batch and generate more interesting text.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNzoDwg2PX9Q"
      },
      "source": [
        "EMBEDDING_DIM = 512\n",
        "\n",
        "def lstm_model(seq_len=100, batch_size=None, stateful=True):\n",
        "  \"\"\"Language model: predict the next word given the current word.\"\"\"\n",
        "  source = tf.keras.Input(\n",
        "      name='seed', shape=(seq_len,), batch_size=batch_size, dtype=tf.int32)\n",
        "\n",
        "  embedding = tf.keras.layers.Embedding(input_dim=256, output_dim=EMBEDDING_DIM)(source)\n",
        "  lstm_1 = tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True)(embedding)\n",
        "  lstm_2 = tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True)(lstm_1)\n",
        "  predicted_char = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(256, activation='softmax'))(lstm_2)\n",
        "  return tf.keras.Model(inputs=[source], outputs=[predicted_char])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgTIeuUERUty"
      },
      "source": [
        "### Train Model\n",
        "First, we create a distribution strategy that can use the TPU. \n",
        "In this case it is TPUStrategy. So we can create and compile the model inside its scope. Once that is done, future calls to the standard Keras methods fit, evaluate and predict use the TPU.\n",
        "\n",
        "Again we train with stateful=False because while training, we only care about one batch at a time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRimZuWQTDnp"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' +\n",
        "                                                             os.environ['COLAB_TPU_ADDR'])\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "# This is the TPU initialization code that has to be at the beginning.\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
        "\n",
        "strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
        "\n",
        "with strategy.scope():\n",
        "  training_model = lstm_model(seq_len=100, stateful=False)\n",
        "  training_model.compile(\n",
        "      optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.01),\n",
        "      loss='sparse_categorical_crossentropy',\n",
        "      metrics=['sparse_categorical_accuracy'])\n",
        "\n",
        "training_model.fit(\n",
        "    input_fun(),\n",
        "    steps_per_epoch=100,\n",
        "    epochs=10\n",
        ")\n",
        "training_model.save_weights('/tmp/bard.h5', overwrite=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V90Pa7veWT3e",
        "outputId": "4d901f4d-54e2-4787-e261-5749bdde2cd2"
      },
      "source": [
        "\n",
        "training_model.fit(\n",
        "    input_fun(),\n",
        "    steps_per_epoch=100,\n",
        "    epochs=10\n",
        ")\n",
        "training_model.save_weights('/tmp/bard.h5', overwrite=True)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "100/100 [==============================] - 12s 62ms/step - loss: 3.4168 - sparse_categorical_accuracy: 0.1710\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 7s 65ms/step - loss: 2.4804 - sparse_categorical_accuracy: 0.3016\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 7s 65ms/step - loss: 1.7265 - sparse_categorical_accuracy: 0.4867\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 7s 66ms/step - loss: 1.4103 - sparse_categorical_accuracy: 0.5695\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 7s 66ms/step - loss: 1.3021 - sparse_categorical_accuracy: 0.5964\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 6s 62ms/step - loss: 1.2452 - sparse_categorical_accuracy: 0.6116\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 7s 66ms/step - loss: 1.2114 - sparse_categorical_accuracy: 0.6206\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 7s 66ms/step - loss: 1.1916 - sparse_categorical_accuracy: 0.6256\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 7s 66ms/step - loss: 1.1729 - sparse_categorical_accuracy: 0.6306\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 7s 65ms/step - loss: 1.1575 - sparse_categorical_accuracy: 0.6346\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF8WYFhHVjd7"
      },
      "source": [
        "###Predictions\n",
        "Now we will use the trained model to make predictions and generate Shakespeare-esque play. Start the model with a *seed* sentence, then generated 250 characters from it. The model makes five predictions from the initial seed. \n",
        "\n",
        "The predictions are done on the CPU so that batch sies (5) in this case does not have to be divisible by 8.\n",
        "\n",
        "Note, now we wull set ```stateful = True``` to keep model's state in between batches. If ```stateful = False```, the model state is reset between each batch, and the model will only be able to use the information from the current batch (a single character) to make a prediction.\n",
        "\n",
        "The output of the model is a set of probabilities for the next character (given the input so far). To build a paragraph, we predict one character at a time and sample a character (based on the probabilities provided by the model). For example, if the input character is \"o\" and the output probabilities are \"p\" (0.65), \"t\" (0.30), others characters (0.05), then we allow our model to generate text other than just \"Ophelia\" and \"Othello.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2eqPEUwX7Fi",
        "outputId": "ced27956-4c96-458b-fbef-3570b5e43358"
      },
      "source": [
        "BATCH_SIZE = 5\n",
        "PREDICT_LEN = 250\n",
        "\n",
        "# Keras requires the batch size be specified ahead of time for stateful models.\n",
        "# We use a sequence length of 1, as we will be feeding in one character at a \n",
        "# time and predicting the next character.\n",
        "prediction_model = lstm_model(seq_len=1, batch_size=BATCH_SIZE, stateful=True)\n",
        "prediction_model.load_weights('/tmp/bard.h5')\n",
        "\n",
        "# We seed the model with our initial string, copied BATCH_SIZE times\n",
        "seed_txt = 'Looks it not like the king?  Verily, we must go! '\n",
        "seed = transform(seed_txt)\n",
        "seed = np.repeat(np.expand_dims(seed, 0), BATCH_SIZE, axis=0)\n",
        "\n",
        "# First, run the seed forward to prime the state of the model.\n",
        "prediction_model.reset_states()\n",
        "for i in range(len(seed_txt) - 1):\n",
        "  prediction_model.predict(seed[:, i : i + 1])\n",
        "\n",
        "#Now we can accumulate predictions\n",
        "predictions = [seed[:, -1:]]\n",
        "for i in range(PREDICT_LEN):\n",
        "  last_word = predictions[-1]\n",
        "  next_probits = prediction_model.predict(last_word)[:, 0, :]\n",
        "\n",
        "# sample from our output distribution\n",
        "  next_idx = [\n",
        "      np.random.choice(256, p=next_probits[i])\n",
        "      for i in range(BATCH_SIZE)\n",
        "  ]\n",
        "  predictions.append(np.asarray(next_idx, dtype=np.int32))\n",
        "  \n",
        "\n",
        "for i in range(BATCH_SIZE):\n",
        "  print('PREDICTION %d\\n\\n' % i)\n",
        "  p = [predictions[j][i] for j in range(PREDICT_LEN)]\n",
        "  generated = ''.join([chr(c) for c in p])  # Convert back to text\n",
        "  print(generated)\n",
        "  print()\n",
        "  assert len(generated) == PREDICT_LEN, 'Generated text too short'"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PREDICTION 0\n",
            "\n",
            "\n",
            " Fear\r\n",
            "thee. Here is the virginities to come abated in\r\n",
            "    great Cupid. Come, come, it will I scouple it.\r\n",
            "  CELIA. The best, be satisfied? He comes in for,\r\n",
            "    And give you with your Grace.\r\n",
            "  CELIA. Fear not to winnot. I will give they art.\r\n",
            "  CE\n",
            "\n",
            "PREDICTION 1\n",
            "\n",
            "\n",
            " Art not conscience,\r\n",
            "And both how we for whats single\r\n",
            "But I yet met, and good for him.\r\n",
            "\r\n",
            "DAUPHIN.\r\n",
            "Thou drawn\r\n",
            "For the beauteous occusator Hercules,\r\n",
            "No, nor off death, thou canst sing; and I\r\n",
            "she flushing that I spoke,\r\n",
            "To such a thing with a lam\n",
            "\n",
            "PREDICTION 2\n",
            "\n",
            "\n",
            " Can you sew?\r\n",
            "Say I make Rome, or shakes, you do beseech you what I think one quarter. But, good friend,\r\n",
            "The Volsces pack.\r\n",
            "\r\n",
            "ROSENCRANTZ and ALuAR.\r\n",
            "In countryeed again; and there died we weigh him in the slave, and that adost you grow for,\r\n",
            "Do as\n",
            "\n",
            "PREDICTION 3\n",
            "\n",
            "\n",
            " And now thou art\r\n",
            "I would go hell.\r\n",
            "\r\n",
            "CLOTEN.\r\n",
            "Faith, sir, thou fortune with himself,\r\n",
            "I have rouse that thou art bigger: to the oizing,\r\n",
            "To set fairer in their pate of Jupiters thing?\r\n",
            "Dost thou a monument?\r\n",
            "So comes with my passes.\r\n",
            "\r\n",
            "CLOTEN.\r\n",
            "To \n",
            "\n",
            "PREDICTION 4\n",
            "\n",
            "\n",
            " I am an act broken deeds,\r\n",
            "    And brought twice I in all the honour of no remembrance, and he\r\n",
            "    Such a hell, contrary, whether you can presume that with our!\r\n",
            "    Are you'll not go about to contract noison?\r\n",
            "    And whereat none of wrinkle.\r\n",
            "  C\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8tGcQ8_a5I4"
      },
      "source": [
        "Reference: https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/shakespeare_with_tpu_and_keras.ipynb#scrollTo=2a5cGsSTEBQD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sL59nF9na7Fw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}