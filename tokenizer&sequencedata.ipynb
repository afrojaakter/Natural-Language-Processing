{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HeadlineSentimentAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afrojaakter/Natural-Language-Processing/blob/main/tokenizer%26sequencedata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYL9rFAQ74kW"
      },
      "source": [
        "###Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaCMcjMQifQc",
        "outputId": "c60a3765-cda6-4314-9d71-de8c996cf3b2"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentences = [\n",
        "    'i love my dog',\n",
        "    'I, love my cat',\n",
        "    'You love my dog!',\n",
        "    'How are you?'\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 2)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'you': 5, 'cat': 6, 'how': 7, 'are': 8}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZG4xny8D7bUH"
      },
      "source": [
        "We see there are we have one token for each word. we also notice that word 'dog' and 'dog!', tokeniziser can tell they are the same word and not putting different token for 'dog' and 'dog!'. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y199A9il5wr6",
        "outputId": "61a03a2d-2974-485e-b22a-8fabdd355ffd"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentences = [\n",
        "             'I love to eat rice!',\n",
        "             'I want to go home',\n",
        "             'I feel scared of animal',\n",
        "             'I like to sleep'\n",
        "             ]\n",
        "tokenizer = Tokenizer(num_words = 100) #to keep the most frequent 100 words if we lave a lot of words, e.g. a book or many book\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'i': 1, 'to': 2, 'love': 3, 'eat': 4, 'rice': 5, 'want': 6, 'go': 7, 'home': 8, 'feel': 9, 'scared': 10, 'of': 11, 'animal': 12, 'like': 13, 'sleep': 14}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmMeRe1C7Jwu"
      },
      "source": [
        "We see there are four i but we get only one token for the the word 'I', one token for word 'to' and so on. i.e. one word has one token number. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Z-qna6s7_xP"
      },
      "source": [
        "###Turning sentences into data\n",
        "Now we will take the sequences of numbers from the sentences and use tools to precess them and train a neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OC4OtmlX8IMQ",
        "outputId": "581baf87-3d9e-4cc0-c143-555e8332a0cb"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "sentences = [\n",
        "             'I love my dog',\n",
        "             'I love my cat',\n",
        "             'Do you love my cat!',\n",
        "             'Do you think my cat is amazing?'\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer(num_words=100, oov_token=\"<OOV>\") #oov_token put token number '1' for a unseen word in the test data\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "sequence = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "padded = pad_sequences(sequence)\n",
        "print(\"\\nWord Index = \", word_index)\n",
        "print(\"\\nSequence = \", sequence)\n",
        "print(\"\\nPadded Sequence =\\n\", padded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Word Index =  {'<OOV>': 1, 'my': 2, 'love': 3, 'cat': 4, 'i': 5, 'do': 6, 'you': 7, 'dog': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
            "\n",
            "Sequence =  [[5, 3, 2, 8], [5, 3, 2, 4], [6, 7, 3, 2, 4], [6, 7, 9, 2, 4, 10, 11]]\n",
            "\n",
            "Padded Sequence =\n",
            " [[ 0  0  0  5  3  2  8]\n",
            " [ 0  0  0  5  3  2  4]\n",
            " [ 0  0  6  7  3  2  4]\n",
            " [ 6  7  9  2  4 10 11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObOci1vDB3HY",
        "outputId": "f84f5afe-8474-4cc8-e154-2ca9d1dbe312"
      },
      "source": [
        "test_sentence = [\n",
        "                 'I really love play with my cat',\n",
        "                 'My cat loves my me'\n",
        "]\n",
        "test_sequence = tokenizer.texts_to_sequences(test_sentence)\n",
        "padded = pad_sequences(test_sequence, maxlen= 10)\n",
        "print(\"\\nTest sequence = \", test_sequence)\n",
        "print(\"\\nPadded test sequence = \\n\", padded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test sequence =  [[5, 1, 3, 1, 1, 2, 4], [2, 4, 1, 2, 1]]\n",
            "\n",
            "Padded test sequence = \n",
            " [[0 0 0 5 1 3 1 1 2 4]\n",
            " [0 0 0 0 0 2 4 1 2 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDPUx4yVDCu3"
      },
      "source": [
        "We see here for each unseen word we got token number '1'. words 'really', 'with', 'me', all has token number '1'. \n",
        "maxlen = 10 makes the row lenght 10 for each row in the padded matrix. If not declared then row length will be the len of largest sentence.  \n",
        "'0' means no token number. "
      ]
    }
  ]
}